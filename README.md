<p align="center">
  <h1 align="center">A Collection of Video Generation Studies</h1>

This GitHub repository summarizes papers and resources related to the video generation task. 

In `reference.bib`, bibtex references are summarized with up-to-date video generation papers, as well as widely used datasets and toolkits.
All refereces are constructed in the form of `author-etal-year-nickname`.

If you have any suggestions about this repository, please feel free to [start a new issue](https://github.com/AlonzoLeeeooo/awesome-video-generation/issues/new) or [pull requests](https://github.com/AlonzoLeeeooo/awesome-video-generation/pulls).

<!-- omit in toc -->
# Contents
- [Products](#products)
  - [Year 2023](#product-year-2023)
- [Papers](#papers)
  - [Year 2023](#papers-year-2023)

<!-- omit in toc -->
# Products
- <span id="product-year-2023">**Year 2023**</span>
  - Animate Anyone [[paper]](https://arxiv.org/pdf/2311.17117.pdf) [[GitHub]](https://github.com/HumanAIGC/AnimateAnyone) [[website]](https://humanaigc.github.io/animate-anyone/)
  - Emu [[website]](https://emu-video.metademolab.com/)
  - Gen-2 [[website]](https://research.runwayml.com/gen2)
  - Gen-1  [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf) [[website]](https://research.runwayml.com/gen1)
  - Midjourney [[website]](https://www.midjourney.com/)
  - Morph Studio [[website]](https://www.morphstudio.com/)
  - Outfit Anyone [[website]](https://humanaigc.github.io/outfit-anyone/)
  - Pika [[website]](https://pika.art/login) 
  - PixelDance [[website]](https://makepixelsdance.github.io/)
  - Stable Video Diffusion [[paper]](https://arxiv.org/pdf/2311.15127.pdf) [[website]](https://stability.ai/news/stable-video-diffusion-open-ai-video-model)
  - VideoPoet [[website]](https://sites.research.google/videopoet/)

<!-- omit in toc -->
# Papers
- <span id="papers-year-2023">**Year 2023**</span>
  - **CVPR**
    - Align your Latents: High-resolution Video Synthesis with Latent Diffusion Models [[paper]](https://arxiv.org/pdf/2304.08818.pdf) [[project]](https://research.nvidia.com/labs/toronto-ai/VideoLDM/) [[reproduced code]](https://github.com/srpkdyy/VideoLDM)
    - Text2Video-Zero: Text-to-image Diffusion Models are Zero-shot Video Generators [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf) [[code]](https://github.com/Picsart-AI-Research/Text2Video-Zero) [[demo]](https://huggingface.co/spaces/PAIR/Text2Video-Zero) [[project]](https://text2video-zero.github.io/) 
    - Video Probabilistic Diffusion Models in Projected Latent Space [[paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf) [[code]](https://github.com/sihyun-yu/PVDM)
  - **NeurIPS**
    - Video Diffusion Models [[paper]](https://arxiv.org/pdf/2204.03458.pdf) [[project]](https://video-diffusion.github.io/)
  - **ICCV**
    - Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf) [[project]](https://research.nvidia.com/labs/dir/pyoco/)
    - Structure and Content-guided Video Synthesis with Diffusion Models [[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf) [[project]](https://research.runwayml.com/gen1)
  - **ICLR**
    - CogVideo: Large-scale Pretraining for Text-to-video Generation via Transformers [[paper]](https://openreview.net/pdf?id=rB6TpjAuSRy) [[code]](https://github.com/THUDM/CogVideo) [[demo]](https://models.aminer.cn/cogvideo/)
    - Make-A-Video: Text-to-video Generation without Text-video Data [[paper]](https://arxiv.org/pdf/2209.14792.pdf) [[project]](https://makeavideo.studio/) [[reproduced code]](https://github.com/lucidrains/make-a-video-pytorch)
    - Phenaki: Variable Length Video Generation From Open Domain Textual Description [[paper]](https://openreview.net/pdf/fe8e106a2746992c9c2e658bdc8cb9c89cc5a39a.pdf) [[code]](https://github.com/lucidrains/phenaki-pytorch)
  - **arXiv**
    - Animate Anyone: Consistent and Controllable Image-to-video Synthesis for Character Animation [[paper]](https://arxiv.org/pdf/2311.17117.pdf) [[code]](https://github.com/HumanAIGC/AnimateAnyone) [[project]](https://humanaigc.github.io/animate-anyone/)
    - AnimateDiff: Animate Your Personalized Text-to-image Diffusion Models without Specific Tuning [[paper]](https://openreview.net/pdf?id=Fx2SbBgcte) [[project]](https://animatediff.github.io/)
    - Control-A-Video: Controllable Text-to-video Generation with Diffusion Models [[paper]](https://arxiv.org/pdf/2305.13840.pdf) [[code]](https://github.com/Weifeng-Chen/control-a-video) [[demo]](https://huggingface.co/spaces/wf-genius/Control-A-Video) [[project]](https://arxiv.org/pdf/2305.13840.pdf)
    - ControlVideo: Training-free Controllable Text-to-video Generation [[paper]](https://arxiv.org/pdf/2305.13077.pdf) [[code]](https://github.com/YBYBZhang/ControlVideo)
    - I2VGen-XL: High-quality Image-to-video Synthesis via Cascaded Diffusion Models [[paper]](https://arxiv.org/pdf/2311.04145.pdf) [[code]](https://github.com/ali-vilab/i2vgen-xl) [[project]](https://i2vgen-xl.github.io/)
    - Imagen Video: High Definition Video Generation with Diffusion Models [[paper]](https://arxiv.org/pdf/2210.02303.pdf) 
    - Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-video Generation [[paper]](https://arxiv.org/pdf/2304.08477.pdf) [[project]](https://latent-shift.github.io/)
    - LAVIE: High-quality Video Generation with Cascaded Latent Diffusion Models [[paper]](https://arxiv.org/pdf/2309.15103.pdf) [[code]](https://github.com/Vchitect/LaVie) [[project]](https://vchitect.github.io/LaVie-project/)
    - Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-video Generation [[paper]](https://showlab.github.io/Show-1/assets/Show-1.pdf) [[code]](https://github.com/showlab/Show-1) [[project]](https://showlab.github.io/Show-1/)
    - SimDA: Simple Diffusion Adapter for Efficient Video Generation [[paper]](https://arxiv.org/pdf/2308.09710.pdf) [[code]](https://github.com/ChenHsing/SimDA) [[project]](https://chenhsing.github.io/SimDA/)
    - Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets [[paper]](https://arxiv.org/pdf/2311.15127.pdf) [[code]](https://github.com/Stability-AI/generative-models) [[project]](https://stability.ai/news/stable-video-diffusion-open-ai-video-model)
    - Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer [[paper]](https://arxiv.org/pdf/2305.05464.pdf)
    - VideoComposer: Compositional Video Synthesis with Motion Controllability [[paper]](https://arxiv.org/pdf/2306.02018.pdf) [[code]](https://github.com/ali-vilab/videocomposer) [[project]](https://videocomposer.github.io/)
    - VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-video Generation [[paper]](https://arxiv.org/pdf/2305.10874.pdf)
    - VideoGen: A Reference-guided Latent Diffusion Approach for High Definition Text-to-video Generation [[paper]](https://arxiv.org/pdf/2309.00398.pdf) [[code]](https://videogen.github.io/VideoGen/)

